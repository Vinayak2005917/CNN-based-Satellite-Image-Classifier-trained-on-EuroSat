{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806dbc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\Pictures\\Side projects\\Satellite Image Classification\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6bca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "num_classes = 10  # adjust if EuroSAT has more classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c06abfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/apollo2506/eurosat-dataset?dataset_version_number=6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.04G/2.04G [08:56<00:00, 4.09MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\.cache\\kagglehub\\datasets\\apollo2506\\eurosat-dataset\\versions\\6\n"
     ]
    }
   ],
   "source": [
    "# KaggleHub path to EuroSAT images\n",
    "path = kagglehub.dataset_download(\"apollo2506/eurosat-dataset\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617f7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your CSV\n",
    "csv_path = path+\"/EuroSAT/train.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "img_path = path+\"/EuroSAT/\"+df.iloc[0,1]\n",
    "label_path = path+\"/EuroSAT/\"+str(df.iloc[0,2])\n",
    "\n",
    "# Define transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),    # Resize to 64x64 if needed\n",
    "    transforms.ToTensor()           # Converts to tensor & scales to [0,1]\n",
    "])\n",
    "\n",
    "# Load image and force RGB\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Apply transform\n",
    "tensor_img = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543bea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for EuroSAT\n",
    "class EuroSATDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.df.iloc[idx, 1])  # filename column\n",
    "        label = int(self.df.iloc[idx, 2])  # label column\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Train & test datasets\n",
    "train_dataset = EuroSATDataset(\n",
    "    csv_file=os.path.join(path, \"EuroSAT\", \"train.csv\"),\n",
    "    root_dir=os.path.join(path, \"EuroSAT\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = EuroSATDataset(\n",
    "    csv_file=os.path.join(path, \"EuroSAT\", \"test.csv\"),\n",
    "    root_dir=os.path.join(path, \"EuroSAT\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbedd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN64x64(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN64x64, self).__init__()\n",
    "\n",
    "        # ---- Convolutional layers ----\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)  # 64x64 -> 64x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)                                      # 64x64 -> 64x64\n",
    "        self.pool = nn.MaxPool2d(2, 2)                                                   # 64x64 -> 32x32\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)                                     # 32x32 -> 32x32\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)                                    # 32x32 -> 32x32\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                                                  # 32x32 -> 16x16\n",
    "\n",
    "        # ---- Fully connected layers ----\n",
    "        self.fc1 = nn.Linear(256*16*16, 512)  # flatten\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # Optional dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Second block\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e5a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN64x64(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()            # suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Started!!------\n",
      "Epoch [1/2], Progress: 10%, Avg Loss: 1.9915\n",
      "Epoch [1/2], Progress: 20%, Avg Loss: 1.8110\n"
     ]
    }
   ],
   "source": [
    "print(\"------Started!!------\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(train_loader)\n",
    "    progress_interval = math.ceil(total_batches / 10)  # every 10%\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader, 1):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print progress every 10%\n",
    "        if batch_idx % progress_interval == 0 or batch_idx == total_batches:\n",
    "            avg_loss = running_loss / batch_idx\n",
    "            progress_percent = int(batch_idx / total_batches * 100)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Progress: {progress_percent}%, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"---------DONE!!---------\")\n",
    "print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"eurosat_cnn64x64.pth\")\n",
    "print(\"Model saved as eurosat_cnn64x64.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test.csv\n",
    "test_csv_path = path + \"/EuroSAT/test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Pick a random index\n",
    "rand_idx = random.randint(0, len(test_df) - 1)\n",
    "\n",
    "# Get file path & label\n",
    "img_path = os.path.join(path, \"EuroSAT\", test_df.iloc[rand_idx, 1])\n",
    "true_label = test_df.iloc[rand_idx, 2]          # integer label\n",
    "class_name = test_df.iloc[rand_idx, 3]          # class name column\n",
    "\n",
    "# Load and preprocess image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = transform(img).unsqueeze(0)  # [1, 3, 64, 64]\n",
    "\n",
    "# Put model in eval mode and run prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)  # raw logits\n",
    "    probs = torch.softmax(outputs, dim=1).squeeze()  # remove batch dim\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_label = predicted.item()\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"True: {class_name} ({true_label})\\nPredicted: {predicted_label}\")\n",
    "plt.show()\n",
    "\n",
    "# Print probabilities in \"index : percentage%\" format\n",
    "print(\"\\nClass probabilities:\")\n",
    "for idx, prob in enumerate(probs):\n",
    "    print(f\"{idx} : {prob.item() * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ac72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Accuracy check\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # no .data needed\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
